import pandas as pd

# snakemake --use-conda --keep-going --cluster-config config/cluster.yaml --cluster 'sbatch --exclude=cn[01-05] -t {cluster.time} --mem={cluster.mem} -c {cluster.cpus} -N {cluster.nodes} -o {cluster.output}' -j 40

cohort_name = "batch3"
PATH_to_samples = "/groups/wyattgrp/users/amunzur/pipeline/resources/sample_lists/" + cohort_name + ".tsv"
configfile: "config/config.yaml"

samples = pd.read_csv(PATH_to_samples, sep = '\t')["sample_names"].tolist()
# samples = ["GUBB-18-029_gDNA_Baseline_IDT_2018Apr18"]

# samples = ["GUBB-18-370-gDNA-Baseline-IDT-2018Aug28_S29", "GUBB-17-012-gDNA-Baseline-IDT-2017Jan17_S1", "GUBB-18-323-gDNA-Baseline-IDT-2018Jul31_S28"]

pair1 = [sample + "_R1" for sample in samples]
pair2 = [sample + "_R2" for sample in samples]

all_pairs = pair1 + pair2

PATH_hg38 = config["PATH_hg38"]
PATH_bed = config["PATH_bed"]
PATH_bg = config["PATH_bg"]
PATH_bets = config["PATH_bets"]

# data files
DIR_raw_fastq = config["DIR_raw_fastq"]
DIR_merged_fastq = config["DIR_merged_fastq"]
DIR_masked_fastq = config["DIR_masked_fastq"]
DIR_trimmed_fastq = config["DIR_trimmed_fastq"]
DIR_extracted_fastq = config["DIR_extracted_fastq"]
DIR_bams = config["DIR_bams"]

# fastqc reports 
DIR_merged_fastqc = config["DIR_merged_fastqc"]
DIR_trimmed_fastqc = config["DIR_trimmed_fastqc"]

# metrics files 
DIR_insertsize_metrics = config["DIR_insertsize_metrics"]
DIR_markdup_metrics = config["DIR_markdup_metrics"]
DIR_markdup_perc_metrics = config["DIR_markdup_perc_metrics"]
DIR_depth_metrics = config["DIR_depth_metrics"]
DIR_depth_metrics_dedup = config["DIR_depth_metrics_dedup"]
DIR_mpileup = config["DIR_mpileup"]
DIR_complexity_metrics = config["DIR_complexity_metrics"]
DIR_dedup_metrics = config["DIR_dedup_metrics"]
DIR_readcounts_metrics = config["DIR_readcounts_metrics"]

# variant calling
DIR_varscan = config["DIR_varscan"]
VarScan_snv = config["VarScan_snv"]
VarScan_indel = config["VarScan_indel"]
DIR_Vardict = config["DIR_Vardict"]
DIR_Mutect = config["DIR_Mutect"]

# annotating variants from varscan
ANNOVAR_snv_input = config["ANNOVAR_snv_input"]
ANNOVAR_indel_input = config["ANNOVAR_indel_input"]
ANNOVAR_snv_output = config["ANNOVAR_snv_output"]
ANNOVAR_indel_output = config["ANNOVAR_indel_output"]

ANNOVAR_Vardict_input = config["ANNOVAR_Vardict_input"]
ANNOVAR_Vardict_output = config["ANNOVAR_Vardict_output"]

# figures 
DIR_insertsize_figures = config["DIR_insertsize_figures"]
DIR_insertsize_figures_PNG = config["DIR_insertsize_figures_PNG"]

###################################################
# TARGET FILES FOR RULE ALL
###################################################
MERGE_R1 = expand(DIR_merged_fastq + "/{cohort_wildcard}/{wildcard}_R1.fastq", wildcard = samples, cohort_wildcard = cohort_name)
MERGE_R2 = expand(DIR_merged_fastq + "/{cohort_wildcard}/{wildcard}_R2.fastq", wildcard = samples, cohort_wildcard = cohort_name)

RUN_FastQC_merged = expand(DIR_merged_fastqc + "/{cohort_wildcard}/{wildcard}_fastqc.html", wildcard = all_pairs, cohort_wildcard = cohort_name)
MASK_FastQ = expand(DIR_masked_fastq + "/{cohort_wildcard}/{wildcard}_masked.fastq", wildcard = all_pairs, cohort_wildcard = cohort_name)

EXTRACT_FastQ_R1 = expand(DIR_extracted_fastq + "/{cohort_wildcard}/{wildcard}_R1_extracted.fastq", wildcard = samples, cohort_wildcard = cohort_name)
EXTRACT_FastQ_R2 = expand(DIR_extracted_fastq + "/{cohort_wildcard}/{wildcard}_R2_extracted.fastq", wildcard = samples, cohort_wildcard = cohort_name)

TRIM_FastQ = [expand(DIR_trimmed_fastq + "/{cohort_wildcard}/{wildcard}_R1_extracted_val_1.fq", wildcard = samples, cohort_wildcard = cohort_name), \
	expand(DIR_trimmed_fastq + "/{cohort_wildcard}/{wildcard}_R2_extracted_val_2.fq", wildcard = samples, cohort_wildcard = cohort_name), \
	expand(DIR_trimmed_fastqc + "/{cohort_wildcard}/{wildcard}_R1_extracted_val_1_fastqc.html", wildcard = samples, cohort_wildcard = cohort_name), \
	expand(DIR_trimmed_fastqc + "/{cohort_wildcard}/{wildcard}_R2_extracted_val_2_fastqc.html", wildcard = samples, cohort_wildcard = cohort_name)]

# Alignment, sorting, marking duplicates and adding read groups
ALIGN_and_SORT = expand(DIR_bams + "/{cohort_wildcard}/sorted/{wildcard}.bam", wildcard = samples, cohort_wildcard = cohort_name)
INDEX_sorted_bams = expand(DIR_bams + "/{cohort_wildcard}/sorted/{wildcard}.bam.bai", wildcard = samples, cohort_wildcard = cohort_name)
DEDUP_umitools = expand(DIR_bams + "/{cohort_wildcard}/dedup/{wildcard}.bam", wildcard = samples, cohort_wildcard = cohort_name)

FIXMATE = expand(DIR_bams + "/{cohort_wildcard}/fixmate/{wildcard}.bam", wildcard = samples, cohort_wildcard = cohort_name)
MARKDUP = [expand(DIR_bams + "/{cohort_wildcard}/markdup/{wildcard}.bam", wildcard = samples, cohort_wildcard = cohort_name), \
	expand(DIR_markdup_metrics + "/{cohort_wildcard}/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name)]
SC_PENALTY = expand(DIR_bams + "/{cohort_wildcard}/SC_penalty/{wildcard}.bam", wildcard = samples, cohort_wildcard = cohort_name)
ADD_RG = expand(DIR_bams + "/{cohort_wildcard}/readGroup/{wildcard}.bam", wildcard = samples, cohort_wildcard = cohort_name)
INDEX_readGroup_bams = expand(DIR_bams + "/{cohort_wildcard}/readGroup/{wildcard}.bam.bai", wildcard = samples, cohort_wildcard = cohort_name)
INDEX_SC_penalty_bams = expand(DIR_bams + "/{cohort_wildcard}/SC_penalty/{wildcard}.bam.bai", wildcard = samples, cohort_wildcard = cohort_name)
run_complexity_metrics_markdup = expand(DIR_complexity_metrics + "/{cohort_wildcard}/PICARD_complexity_dedup/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name)
RUN_fastq_read_counts = expand(DIR_readcounts_metrics + "/raw/{cohort_wildcard}/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name)
RUN_extract_markdup_perc = expand(DIR_markdup_perc_metrics + "/{cohort_wildcard}/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name)

RUN_mpileup = expand(DIR_mpileup + "/{cohort_wildcard}/{wildcard}.mpileup", wildcard = samples, cohort_wildcard = cohort_name)
RUN_varscan_snv = expand(VarScan_snv + "/{cohort_wildcard}/{wildcard}.vcf", wildcard = samples, cohort_wildcard = cohort_name)
RUN_VarScan_indel = expand(VarScan_indel + "/{cohort_wildcard}/{wildcard}.vcf", wildcard = samples, cohort_wildcard = cohort_name)
RUN_Vardict = expand(DIR_Vardict + "/{cohort_wildcard}/{wildcard}.vcf", wildcard = samples, cohort_wildcard = cohort_name)
REFORMAT_vardict_results = expand(DIR_Vardict + "/{cohort_wildcard}_reformatted/{wildcard}.tsv", wildcard = samples, cohort_wildcard = cohort_name)

RUN_Mutect2 = expand(DIR_Mutect + "/raw/{cohort_wildcard}/{wildcard}_vcf.gz", wildcard = samples, cohort_wildcard = cohort_name)
FILTER_Mutect2 = expand(DIR_Mutect + "/filtered/{cohort_wildcard}/{wildcard}_vcf.gz", wildcard = samples, cohort_wildcard = cohort_name)

make_ANNOVAR_snv_input = expand(ANNOVAR_snv_input + "/{cohort_wildcard}/{wildcard}_anno.tsv", wildcard = samples, cohort_wildcard = cohort_name)
make_ANNOVAR_indel_input = expand(ANNOVAR_indel_input + "/{cohort_wildcard}/{wildcard}_anno.tsv", wildcard = samples, cohort_wildcard = cohort_name)
make_ANNOVAR_vardict_input = expand(ANNOVAR_Vardict_input + "/{cohort_wildcard}/{wildcard}_anno.tsv", wildcard = samples, cohort_wildcard = cohort_name)

run_ANNOVAR_snv = expand(ANNOVAR_snv_output + "/{cohort_wildcard}/{wildcard}.hg38_multianno.txt", wildcard = samples, cohort_wildcard = cohort_name)
run_ANNOVAR_indel = expand(ANNOVAR_indel_output + "/{cohort_wildcard}/{wildcard}.hg38_multianno.txt", wildcard = samples, cohort_wildcard = cohort_name)
run_ANNOVAR_vardict = expand(ANNOVAR_Vardict_output + "/{cohort_wildcard}/{wildcard}.hg38_multianno.txt", wildcard = samples, cohort_wildcard = cohort_name)

run_insert_size = [expand(DIR_insertsize_metrics + "/{cohort_wildcard}/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name), \
	expand(DIR_insertsize_figures + "/{cohort_wildcard}/{wildcard}.pdf", wildcard = samples, cohort_wildcard = cohort_name)]
PDF_to_PNG = expand(DIR_insertsize_figures_PNG + "/{cohort_wildcard}/{wildcard}.png", wildcard = samples, cohort_wildcard = cohort_name)
run_depth = expand(DIR_depth_metrics + "/{cohort_wildcard}/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name) # depth at each position
run_depth_umi = expand(DIR_depth_metrics_dedup + "/{cohort_wildcard}/{wildcard}.txt", wildcard = samples, cohort_wildcard = cohort_name)
###################################################
# TARGET RULES
###################################################

# only use expansion here
rule all:
	input:
		# MERGE_R1,
		# MERGE_R2,
		# RUN_FastQC_merged,
		# MASK_FastQ,
		# EXTRACT_FastQ_R1,
		# EXTRACT_FastQ_R2,
		# TRIM_FastQ,
		# ALIGN_and_SORT,
		# INDEX_sorted_bams, 
		# INDEX_readGroup_bams,
		# INDEX_SC_penalty_bams,
		# DEDUP_umitools,
		# FIXMATE,
		# MARKDUP,
		# SC_PENALTY,
		# ADD_RG,
		# RUN_mpileup,
		# RUN_fastq_read_counts, 
		# RUN_extract_markdup_perc,
		# RUN_varscan_snv, 
		# RUN_VarScan_indel, 
		RUN_Mutect2,
		# FILTER_Mutect2,
		# RUN_Vardict,
		# REFORMAT_vardict_results,
		# make_ANNOVAR_snv_input,
		# make_ANNOVAR_indel_input,
		# make_ANNOVAR_vardict_input,
		# run_ANNOVAR_snv,
		# run_ANNOVAR_indel, 
		# run_ANNOVAR_vardict,
		# run_insert_size, 
		# PDF_to_PNG,
		# run_depth, 
		# run_depth_umi

##### Modules #####
include: "rules/process_fastq.smk"
include: "rules/process_bams.smk"
include: "rules/index_bams.smk"
include: "rules/run_metrics.smk"
include: "rules/variant_calling.smk"





